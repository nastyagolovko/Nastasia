## Восстановление центра нормального распределения 
estimateMu <- function(objects) 
{     
## mu = 1 / m  *  sum_{i=1}^m(objects_i)     
rows <- dim(objects)[1]     
cols <- dim(objects)[2]          
mu <- matrix(NA, 1, cols)     
for (col in 1:cols)      
{         
mu[1, col] = mean(objects[,col])     
}          
return(mu) 
} 
 ##  Восстановление ковариационной матрицы нормального распределения 
estimateCovarianceMatrix <- function(objects, mu) 
{     
rows <- dim(objects)[1]     
cols <- dim(objects)[2]     
sigma <- matrix(0, cols, cols)          
for (i in 1:rows)     
{         
sigma <- sigma + (t(objects[i,] - mu) %*% 
(objects[i,] - mu)) / (rows - 1)     
}          
return (sigma) 
} 
 ## Получение коэффициентов подстановочного алгоритма 
getPlugInDiskriminantCoeffs <- function(mu1, sigma1, mu2, sigma2) 
{     
## Line equation: a*x1^2 + b*x1*x2 + c*x2 + d*x1 + e*x2 + f = 0     
invSigma1 <- solve(sigma1) 
    invSigma2 <- solve(sigma2)          
f <- log(abs(det(sigma1))) - log(abs(det(sigma2))) + mu1 %*% invSigma1 %*% t(mu1) - mu2 %*% invSigma2 %*% t(mu2);          alpha <- invSigma1 - invSigma2     a <- alpha[1, 1]     b <- 2 * alpha[1, 2]     c <- alpha[2, 2]          beta <- invSigma1 %*% t(mu1) - invSigma2 %*% t(mu2)      d <- -2 * beta[1, 1]     e <- -2 * beta[2, 1] 
     return (c("x^2" = a, "xy" = b, "y^2" = c, "x" = d, "y" = e, "1" = f)) 
} 
 ## Количество объектов в каждом классе 
ObjectsCountOfEachClass <- 100 
 ## Подключаем библиотеку MASS для генерации многомерного нормального распределения 
library(MASS) 
 ## Генерируем тестовые данные 
Sigma1 <- matrix(c(10, 0, 0, 1), 2, 2) 
Sigma2 <- matrix(c(1, 0, 0, 5), 2, 2) 
 Mu1 <- c(1, 0) 
 Mu2 <- c(15, 0) 
 xy1 <- mvrnorm(n=ObjectsCountOfEachClass, Mu1, Sigma1) 
xy2 <- mvrnorm(n=ObjectsCountOfEachClass, Mu2, Sigma2) 
 ## Собираем два класса в одну выборку 
xl <- rbind(cbind(xy1, 1), cbind(xy2, 2)) 
 ## Рисуем обучающую выборку  
colors <- c(rgb(0/255, 162/255, 232/255), rgb(0/255, 200/255, 0/255)) plot(xl[,1], xl[,2], pch = 21, bg = colors[xl[,3]], asp = 1) 
 ## Оценивание 
objectsOfFirstClass <- xl[xl[,3] == 1, 1:2] 
objectsOfSecondClass <- xl[xl[,3] == 2, 1:2] 
mu1 <- estimateMu(objectsOfFirstClass) 
mu2 <- estimateMu(objectsOfSecondClass) 
 sigma1 <- estimateCovarianceMatrix(objectsOfFirstClass, mu1) 
 sigma2 <- estimateCovarianceMatrix(objectsOfSecondClass, mu2) 
 coeffs <- getPlugInDiskriminantCoeffs(mu1, sigma1, mu2, sigma2) 
 ## Рисуем дискриминантую функцию – красная линия 
x <- y <- seq(-10, 20, len=100)  
z <- outer(x, y, function(x, y) coeffs["x^2"]*x^2 +  coeffs["xy"]*x*y + coeffs["y^2"]*y^2 + coeffs["x"]*x + coeffs["y"]*y + coeffs["1"])  
 contour(x, y, z, levels=0, drawlabels=FALSE, lwd = 3, col = "red", add = TRUE) 
 